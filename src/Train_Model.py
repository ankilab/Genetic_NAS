import tensorflow_addons as tfa
from tensorflow import keras
import tensorflow as tf

if tf.config.list_physical_devices('GPU'):
    strategy = tf.distribute.MirroredStrategy()
else:  # Use the Default Strategy
    strategy = tf.distribute.get_strategy()

'''This function is used for training a model using the TensorFlow Keras API. It takes in a training dataset (
train_ds), a validation dataset (val_ds), the model to be trained, the number of training epochs, and a filepath to 
save the model checkpoint after each epoch. The function also takes in an optimizer which is used to update the 
model's parameters during training. By default, it uses the AdaBelief optimizer. The function also specifies the loss 
function as BinaryFocalCrossentropy and the metrics to be monitored during training is accuracy.

The function also includes a try-except block to handle the case when the model's memory usage exceeds the available 
memory. If this happens, the function will print a message and return None for the history. The function then returns 
the trained model and the history of the training process.'''


def train_model(train_ds, val_ds,
                model, epochs=20,
                checkpoint_filepath="checkpoints/checkpoint"):

    checkpoint_callback = keras.callbacks.ModelCheckpoint(checkpoint_filepath,
                                                          monitor="val_accuracy",
                                                          save_best_only=True,
                                                          save_weights_only=True)

    loss_fn = keras.losses.CategoricalCrossentropy(label_smoothing=0.1)

    opt = tfa.optimizers.LazyAdam(learning_rate=0.002)
    opt = tfa.optimizers.MovingAverage(opt)
    opt = tfa.optimizers.Lookahead(opt)

    model.compile(optimizer=opt,
                  loss=loss_fn,
                  metrics=['accuracy'])

    # TODO: Find a solution to the problem of large transformer sequence.
    #       Which can lead to insufficient Graph Memory.

    with strategy.scope():
        try:
            # Fit the model on the batches generated by datagen.flow().
            history = model.fit(train_ds,
                                epochs=epochs,
                                validation_data=val_ds,
                                callbacks=[checkpoint_callback])

            model.load_weights(checkpoint_filepath)

        except:
            history = None
            print("Do not have enough memory.")

    return model, history
